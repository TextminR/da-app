%!TEX root=./thesis.tex

% Hier finden sich alle Referenzen aus Literatur und Internet, welche innerhalb der eigentlichen Arbeit verwendet wurden.
% Zur einfachen Anwendung werden hier einige Beispiele gegeben.

% Nutzung einer Quelle, welche über das Internet zugänglich ist.
% Einige Parameter sollten bei jeder Referenz vorhanden sein:
% - title: 			Der Titel der Website (wird meist in der Adresszeile angezeigt)
% - author:			Alle Autoren, welche als für den Text verantwortlich gegeben sind. Sollte keiner angegeben sein, könnte etwa „Autoren von {Seitenname}“ verwendet werden
% - url:			Ein eindeutiger Link zur referenzierten Quelle, welcher im Optimalfall über mehrere Jahre hinweg verfügbar ist
% - note:			Ebenfalls sollte das Datum des letzten Zugriffs in einer Notiz angegeben
@online{ wiki:syt,
	title = {Systemtechnik},
	author = {Wikipedia Autoren},
	url = {https://de.wikipedia.org/wiki/Systemtechnik},
	organization = {Wikipedia},
	note = {07.03.2018}
}

% Nutzung von Inhalten eines Buchs.
% Bei Büchern gibt es einige Angaben, welche auf jeden Fall angegeben werden müssen:
% - title: 		Der Vollständige Titel des Buchs
% - author: 	Alle beteiligten Autoren
% - year:		Erscheinungsdatum der verwendeten Auflage (siehe auch Parameter „edition“)
% - edition:	Aktuelle Auflage des Buchs (wenn nicht angegeben: 1. Auflage verwenden) und Markierung spezieller Versionen
% - publisher:	Der Verlag, welcher das Buch herausgab (häufig werden vom Verlag noch kleiner Änderungen vorgenommen)
% - pages:		Alle Seiten, durch Komma getrennt, welche innerhalb der Arbeit verwendet wurden (mehrere Seiten können auch mit „-“ angegeben werden)
%
% Optional sind auch folgende Parameter interessant:
% - subtitle:	Falls Bücher sich etwa nur durch ihren Untertitel unterscheiden
% - pagetotal:	Gesamte Anzahl der Seiten der verwendeten Quelle
@book{ physik1,
	title = {Physik 1},
	author = {Schweitzer, Christian and Svoboda, Peter and Trieb, Lutz},
	year = {2011},
	subtitle = {Mechanik, Thermodynamik, Optik},
	edition = {7. Auflage},
	publisher = {Veritas},
	pages = {140, 145-150},
	pagetotal = {296}
}
%--------------------------------------------------------------%
%------------- Anfang von dlengsteiner's Quellen --------------%
%--------------------------------------------------------------%

@article{Kherwa2020,
   author = {Pooja Kherwa and Poonam Bansal},
   doi = {10.4108/eai.13-7-2018.159623},
   issn = {20329407},
   issue = {24},
   journal = {EAI Endorsed Transactions on Scalable Information Systems},
   pages = {3-6},
   title = {Topic Modeling: A Comprehensive Review},
   volume = {7},
   year = {2020},
}

@article{Alghamdi2015,
   author = {Rubayyi Alghamdi and Khalid Alfalqi},
   doi = {10.14569/ijacsa.2015.060121},
   issn = {2158107X},
   issue = {1},
   journal = {International Journal of Advanced Computer Science and Applications},
   title = {A Survey of Topic Modeling in Text Mining},
   volume = {6},
   year = {2015},
}

@book{Plaue2021,
   author = {Matthias Plaue},
   doi = {10.1007/978-3-662-63489-9},
   journal = {Data Science},
   pages = {185-245},
   title = {Data Science},
   url = {https://www.springerprofessional.de/content/pdfId/25996024/10.1007/978-3-662-67882-4_6},
   year = {2021},
}

@misc{Wiedemann2022-tm-R,
   author = {Gregor Wiedemann},
   doi = {10.5771/1615-634X-2022-3-286},
   issn = {1615634X},
   issue = {3},
   journal = {Medien und Kommunikationswissenschaft},
   title = {The World of Topic Modeling in R},
   volume = {70},
   year = {2022},
}

@misc{gutenbergr,
   author = {David Robinson},
   journal = {RDocumentation},
   month = {12},
   title = {gutenbergr: R package to search and download public domain texts from Project Gutenberg},
   url = {https://www.rdocumentation.org/packages/gutenbergr/versions/0.2.3},
   year = {2022},
}

@article{topicmodelsurvey_padmaja,
   author = {Padmaja CH V R},
   city = {Udaipur},
   doi = {10.26483/ijarcs.v9i3.6107},
   issn = {0976-5697},
   issue = {3},
   journal = {International journal of advanced research in computer science},
   keywords = {Algorithms,Classification,Computer science,Data mining,Methods,Modelling,Semantic analysis,Semantics,Statistical analysis},
   pages = {173-177},
   publisher = {International Journal of Advanced Research in Computer Science},
   title = {PROBABILISTIC TOPIC MODELING AND ITS VARIANTS – A SURVEY},
   volume = {9},
   year = {2018},
}

% NEEDS TO BE ADDED TO MENDELEY
@misc{sense-topic-modelling-van-kessel,
    author = {Patrick Van Kessel},
    publisher = {Pew Research Center: Decoded},
    journal = {Pew Research Center: Decoded},
    day = {13},
    month = {8},
    year = {2017},
    title = {Making sense of topic models},
    url = {https://medium.com/pew-research-center-decoded/making-sense-of-topic-models-953a5e42854e},
}


% VLLT AUCH FÜR SNYDER

@misc{soares2023embeddings,
   author = {Liliane Soares da Costa and Italo L. Oliveira and Renato Fileto},
   doi = {10.1007/s10115-023-01856-z},
   issn = {02193116},
   issue = {7},
   journal = {Knowledge and Information Systems},
   title = {Text classification using embeddings: a survey},
   volume = {65},
   year = {2023},
}


@article{amjad2023sentiment,
   author = {Tamara Amjad Al-Qablan and Mohd Halim Mohd Noor and Mohammed Azmi Al-Betar and Ahamad Tajudin Khader},
   doi = {10.1007/s00521-023-08941-y},
   issn = {0941-0643},
   issue = {29},
   journal = {Neural Computing and Applications},
   month = {10},
   pages = {21567-21601},
   title = {A survey on sentiment analysis and its applications},
   volume = {35},
   year = {2023},
}


@article{Wankhade2022sentiment,
   author = {Mayur Wankhade and Annavarapu Chandra Sekhara Rao and Chaitanya Kulkarni},
   doi = {10.1007/s10462-022-10144-1},
   issn = {15737462},
   issue = {7},
   journal = {Artificial Intelligence Review},
   title = {A survey on sentiment analysis methods, applications, and challenges},
   volume = {55},
   year = {2022},
}

@article{Meibauer2003phrasen,
   author = {Jörg Meibauer},
   doi = {10.1515/zfsw.2003.22.2.153},
   issn = {16133706},
   issue = {2},
   journal = {Zeitschrift fur Sprachwissenschaft},
   title = {Phrasenkomposita zwischen Wortsyntax und Lexikon},
   volume = {22},
   year = {2003},
}



%----------------------------------------------------------%
%------------- Anfang von asuender's Quellen --------------%
%----------------------------------------------------------%

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{falcon,
  title={The Falcon Series of Language Models: Towards Open Frontier Models},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Alhammadi, Maitha and Daniele, Mazzotta and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year={2023}
}

@book{PaaßGerhard2023FMfN,
   author = {Gerhard Paaß and Sven Giesselbach},
   city = {Cham},
   doi = {10.1007/978-3-031-23190-2},
   edition = {1st ed. 2023},
   editor = {Sven Giesselbach},
   isbn = {3031231902},
   keywords = {Artificial Intelligence,Artificial intelligence,Computational Linguistics,Computational linguistics,Expert systems (Computer science),Knowledge Based Systems,Machine Learning,Machine learning,Natural Language Processing (NLP),Natural language processing (Computer science)},
   publisher = {Springer International Publishing Imprint: Springer},
   title = {Foundation Models for Natural Language Processing : Pre-trained Language Models Integrating Media},
   year = {2023},
}

@article{Hu2021,
   author = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
   month = {6},
   title = {LoRA: Low-Rank Adaptation of Large Language Models},
   url = {http://arxiv.org/abs/2106.09685},
   year = {2021},
}

@misc{HugFaceLoRa,
   author = {Hugging Face},
   title = {Low-Rank Adaptation of Large Language Models (LoRA)},
   url = {https://huggingface.co/docs/diffusers/training/lora},
}

@article{Lester2021,
   author = {Brian Lester and Rami Al-Rfou and Noah Constant},
   month = {4},
   title = {The Power of Scale for Parameter-Efficient Prompt Tuning},
   url = {http://arxiv.org/abs/2104.08691},
   year = {2021},
}

@article{Shen2019,
   author = {Sheng Shen and Zhen Dong and Jiayu Ye and Linjian Ma and Zhewei Yao and Amir Gholami and Michael W. Mahoney and Kurt Keutzer},
   month = {9},
   title = {Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT},
   url = {http://arxiv.org/abs/1909.05840},
   year = {2019},
}

@article{Kirkpatrick2017,
   issn = {10916490},
   issue = {13},
   journal = {Proceedings of the National Academy of Sciences of the United States of America},
   keywords = {Artificial intelligence,Continual learning,Deep learning,Stability plasticity,Synaptic consolidation},
   month = {3},
   pages = {3521-3526},
   pmid = {28292907},
   publisher = {National Academy of Sciences},
   title = {Overcoming catastrophic forgetting in neural networks},
   volume = {114},
   year = {2017},
}

@article{Frantar2022,
   author = {Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
   month = {10},
   title = {GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
   url = {http://arxiv.org/abs/2210.17323},
   year = {2022},
}




